{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Multi-head Attention\n",
    "In practical, we hope model could base on the attention mechanism to learn different behaviour as knowledge and combine them together, catching dependency relations among sequences when given same query, key and value. So it might be beneficial if we allow attention to use different representation subspaces of query, key and value and combine them together.\n",
    "So, we could use different linear projections learnt by us independently to transform query, key and value. Then they will be sent into average pooling parallely and concatenated together, and projected by another linear projection, to generate the final output. For each attention pooling, we call it a *head*.\n",
    "![multihead](../statics/imgs/section10.5_fig1.jpg)\n",
    "Each attention head $\\mathbf{h}_i$ could be calculated as\n",
    "$$\n",
    "\\mathbf{h}_i = f(\\mathbf{W}_i^{(q)}\\mathbf{q}, \\mathbf{W}_i^{(k)}\\mathbf{k}, \\mathbf{W}_i^{(v)}\\mathbf{v})\n",
    "$$\n",
    "where $f$ represents average pooling function. It could be additive or dot-product scalar attention."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-14T07:30:53.320267600Z",
     "start_time": "2023-07-14T07:30:51.078840500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def transpose_qkv(X, num_heads):\n",
    "    \"\"\"transpose the shape of input vectors for parallel computation of the multi-head attention\"\"\"\n",
    "    \"\"\"\n",
    "    (batch_size, len(queries or kv pairs), num_hiddens) -> (batch_size, len(queries or kv pairs), num_heads, num_hiddens/num_heads)\n",
    "    \"\"\"\n",
    "    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    # output: (batch_size * num_heads, len(queries or kv pairs), num_hiddens/num_heads)\n",
    "    return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "def transpose_output(X, num_heads):\n",
    "    \"\"\"reverse the operation of transpose_qkv\"\"\"\n",
    "    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    return X.reshape(X.shape[0], X.shape[1], -1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-14T07:45:11.005338600Z",
     "start_time": "2023-07-14T07:45:10.991376100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = d2l.DotProductAttention(dropout)\n",
    "        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)\n",
    "        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)\n",
    "        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)\n",
    "        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        # queries, keys, values: (batch_size, len(queries) or len(kv pairs), num_hiddens)\n",
    "        # valid_lens: (batch_size, len(queries)) or (batch_size, )\n",
    "        queries = transpose_qkv(self.W_q(queries), self.num_heads)\n",
    "        keys = transpose_qkv(self.W_k(keys), self.num_heads)\n",
    "        values = transpose_qkv(self.W_v(values), self.num_heads)\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, repeats=self.num_heads, dim=0)\n",
    "\n",
    "        # output: (batch_size*num_heads, len(queries), num_hiddens/num_heads)\n",
    "        output = self.attention(queries, keys, values, valid_lens)\n",
    "        output_concat = transpose_output(output, self.num_heads)\n",
    "        return self.W_o(output_concat)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-14T07:57:25.566336600Z",
     "start_time": "2023-07-14T07:57:25.557328300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "MultiHeadAttention(\n  (attention): DotProductAttention(\n    (dropout): Dropout(p=0.5, inplace=False)\n  )\n  (W_q): Linear(in_features=100, out_features=100, bias=False)\n  (W_k): Linear(in_features=100, out_features=100, bias=False)\n  (W_v): Linear(in_features=100, out_features=100, bias=False)\n  (W_o): Linear(in_features=100, out_features=100, bias=False)\n)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hiddens, num_heads = 100, 5\n",
    "attention = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens, num_hiddens, num_heads, 0.5)\n",
    "attention.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-14T07:58:07.510243900Z",
     "start_time": "2023-07-14T07:58:07.493649300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 4, 100])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, num_queries = 2, 4\n",
    "num_kvpairs, valid_lens = 6, torch.tensor([3, 2])\n",
    "X = torch.ones((batch_size, num_queries, num_hiddens))\n",
    "Y = torch.ones((batch_size, num_kvpairs, num_hiddens))\n",
    "attention(X, Y, Y, valid_lens).shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-14T07:59:57.682499700Z",
     "start_time": "2023-07-14T07:59:57.628575900Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
