{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Transformer\n",
    "Transformer is completely based on the attention mechanism, without using CNN or RNN. It used in the seq2seq tasks in text data, but have been widely used in many DL tasks, such as language, visual and audio.\n",
    "It is a classic encoder-decoder architecture, which has been shown in the fig below. Compared with seq2seq model we implemented by using Bahdanau attention in section10.4, Transformer's encoder and decoder is stacked by layers of self-attention modules. Origin input sequence and output embedding sequence will add with positional encoding, then put them into encoder and decoder.\n",
    "![transformer](../statics/imgs/section10.7_fig1.jpg)\n",
    "Generally, fig above shows that the encoder of Transformer is consists of many layers with same structure, and each layer owns 2 sublayer. First sublayer is multi-head self-attention pooling, and second sublayer is positionwise feed-forward network. To be concisely, when we execute self-attention calculations in encoder, query, key and value are come from the output of the former encoder layer. In addition, each sublayer uses residual connection.\n",
    "Decoder also stacked with many same layers, and uses residual and layer normalization in the layer as well. Expect from two sublayer we described in the encoder, the third sublayer is inserted into the sublayer as well, which we called encoder-decoder attention layer. In encoder-decoder attention, query comes from the output from the last decoder layer. However, each position in decoder can only consider every position before current position. This type of masked attention keeps attributes of auto-regressive, and make sure the prediction only depends on those generated output tokens."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-15T02:35:05.665961700Z",
     "start_time": "2023-07-15T02:35:02.238570300Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# position-wise feed-forward network\n",
    "class PositionWiseFFN(nn.Module):\n",
    "    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs, **kwargs):\n",
    "        super(PositionWiseFFN, self).__init__(**kwargs)\n",
    "        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dense2(self.relu(self.dense1(X)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T02:37:24.789091700Z",
     "start_time": "2023-07-15T02:37:24.772096700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.4256, -0.0439, -0.2478,  0.1027,  0.3032, -0.6424, -0.0154, -0.6512],\n        [ 0.4256, -0.0439, -0.2478,  0.1027,  0.3032, -0.6424, -0.0154, -0.6512],\n        [ 0.4256, -0.0439, -0.2478,  0.1027,  0.3032, -0.6424, -0.0154, -0.6512]],\n       grad_fn=<SelectBackward0>)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn = PositionWiseFFN(4, 4, 8)\n",
    "ffn.eval()\n",
    "ffn(torch.ones((2, 3, 4)))[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T02:38:38.213075200Z",
     "start_time": "2023-07-15T02:38:38.163957900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer norm: tensor([[-1.0000,  1.0000],\n",
      "        [-1.0000,  1.0000]], grad_fn=<NativeLayerNormBackward0>) \n",
      "batch norm: tensor([[-1.0000, -1.0000],\n",
      "        [ 1.0000,  1.0000]], grad_fn=<NativeBatchNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# residual & normalization\n",
    "ln = nn.LayerNorm(2)\n",
    "bn = nn.BatchNorm1d(2)\n",
    "X = torch.tensor([[1, 2], [2, 3]], dtype=torch.float32)\n",
    "print('layer norm:', ln(X), '\\nbatch norm:', bn(X))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T02:40:56.582838200Z",
     "start_time": "2023-07-15T02:40:56.568875500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, dropout, **kwargs):\n",
    "        super(AddNorm, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(normalized_shape)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        # layernorm after residual\n",
    "        return self.ln(self.dropout(Y) + X)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T02:44:01.090497Z",
     "start_time": "2023-07-15T02:44:01.076535600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 3, 4])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_norm = AddNorm([3, 4], 0.5)\n",
    "add_norm.eval()\n",
    "add_norm(torch.ones((2, 3, 4)), torch.ones((2, 3, 4))).shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T02:44:42.550352600Z",
     "start_time": "2023-07-15T02:44:42.497844700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# encoder\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens,\n",
    "                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias=False, **kwargs):\n",
    "        super(EncoderBlock, self).__init__(**kwargs)\n",
    "        self.attention = d2l.MultiHeadAttention(key_size, query_size, value_size,\n",
    "                                                num_hiddens, num_heads, dropout, use_bias)\n",
    "        self.addNorm1 = AddNorm(norm_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)\n",
    "        self.addNorm2 = AddNorm(norm_shape, dropout)\n",
    "\n",
    "    def forward(self, X, valid_lens):\n",
    "        Y = self.addNorm1(X, self.attention(X, X, X, valid_lens))\n",
    "        return self.addNorm2(Y, self.ffn(Y))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T02:51:06.298568300Z",
     "start_time": "2023-07-15T02:51:06.280399200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 100, 24])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# any layer of encoder will not change the shape of input\n",
    "X = torch.ones((2, 100, 24))\n",
    "valid_lens = torch.tensor([3, 2])\n",
    "encoder_blk = EncoderBlock(24, 24, 24, 24, [100, 24], 24, 48, 8, 0.5)\n",
    "encoder_blk.eval()\n",
    "encoder_blk(X, valid_lens).shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T02:52:27.670120600Z",
     "start_time": "2023-07-15T02:52:27.589821100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class TransformerEncoder(d2l.Encoder):\n",
    "    def __init__(self, vocab_size, key_size, query_size, value_size,\n",
    "                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout,\n",
    "                 use_bias=False, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(\"block\" + str(i),\n",
    "                                 EncoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape,ffn_num_input,\n",
    "                                              ffn_num_hiddens, num_heads, dropout, use_bias))\n",
    "    def forward(self, X, valid_lens, *args):\n",
    "        # as positional encoding is between -1 and 1, so embedding values have to be scaled, then adds with positional code\n",
    "        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n",
    "        self.attention_weights = [None] * len(self.blks)\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X, valid_lens)\n",
    "            self.attention_weights[i] = blk.attention.attention.attention_weights\n",
    "        return X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T03:01:55.079654600Z",
     "start_time": "2023-07-15T03:01:55.046212100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "TransformerEncoder(\n  (embedding): Embedding(200, 24)\n  (pos_encoding): PositionalEncoding(\n    (dropout): Dropout(p=0.5, inplace=False)\n  )\n  (blks): Sequential(\n    (block0): EncoderBlock(\n      (attention): MultiHeadAttention(\n        (attention): DotProductAttention(\n          (dropout): Dropout(p=0.5, inplace=False)\n        )\n        (W_q): Linear(in_features=24, out_features=24, bias=False)\n        (W_k): Linear(in_features=24, out_features=24, bias=False)\n        (W_v): Linear(in_features=24, out_features=24, bias=False)\n        (W_o): Linear(in_features=24, out_features=24, bias=False)\n      )\n      (addNorm1): AddNorm(\n        (dropout): Dropout(p=0.5, inplace=False)\n        (ln): LayerNorm((100, 24), eps=1e-05, elementwise_affine=True)\n      )\n      (ffn): PositionWiseFFN(\n        (dense1): Linear(in_features=24, out_features=48, bias=True)\n        (relu): ReLU()\n        (dense2): Linear(in_features=48, out_features=24, bias=True)\n      )\n      (addNorm2): AddNorm(\n        (dropout): Dropout(p=0.5, inplace=False)\n        (ln): LayerNorm((100, 24), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (block1): EncoderBlock(\n      (attention): MultiHeadAttention(\n        (attention): DotProductAttention(\n          (dropout): Dropout(p=0.5, inplace=False)\n        )\n        (W_q): Linear(in_features=24, out_features=24, bias=False)\n        (W_k): Linear(in_features=24, out_features=24, bias=False)\n        (W_v): Linear(in_features=24, out_features=24, bias=False)\n        (W_o): Linear(in_features=24, out_features=24, bias=False)\n      )\n      (addNorm1): AddNorm(\n        (dropout): Dropout(p=0.5, inplace=False)\n        (ln): LayerNorm((100, 24), eps=1e-05, elementwise_affine=True)\n      )\n      (ffn): PositionWiseFFN(\n        (dense1): Linear(in_features=24, out_features=48, bias=True)\n        (relu): ReLU()\n        (dense2): Linear(in_features=48, out_features=24, bias=True)\n      )\n      (addNorm2): AddNorm(\n        (dropout): Dropout(p=0.5, inplace=False)\n        (ln): LayerNorm((100, 24), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = TransformerEncoder(200, 24, 24, 24, 24, [100, 24], 24, 48, 8, 2, 0.5)\n",
    "encoder.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T03:02:27.133934400Z",
     "start_time": "2023-07-15T03:02:27.085517800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 100, 24])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(torch.ones((2, 100), dtype=torch.long), valid_lens).shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T03:03:11.248344700Z",
     "start_time": "2023-07-15T03:03:11.193123900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# decoder"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
