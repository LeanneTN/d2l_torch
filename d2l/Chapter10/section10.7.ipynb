{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Transformer\n",
    "Transformer is completely based on the attention mechanism, without using CNN or RNN. It used in the seq2seq tasks in text data, but have been widely used in many DL tasks, such as language, visual and audio.\n",
    "It is a classic encoder-decoder architecture, which has been shown in the fig below. Compared with seq2seq model we implemented by using Bahdanau attention in section10.4, Transformer's encoder and decoder is stacked by layers of self-attention modules. Origin input sequence and output embedding sequence will add with positional encoding, then put them into encoder and decoder.\n",
    "![transformer](../statics/imgs/section10.7_fig1.jpg)\n",
    "Generally, fig above shows that the encoder of Transformer is consists of many layers with same structure, and each layer owns 2 sublayer. First sublayer is multi-head self-attention pooling, and second sublayer is positionwise feed-forward network. To be concisely, when we execute self-attention calculations in encoder, query, key and value are come from the output of the former encoder layer. In addition, each sublayer uses residual connection.\n",
    "Decoder also stacked with many same layers, and uses residual and layer normalization in the layer as well. Expect from two sublayer we described in the encoder, the third sublayer is inserted into the sublayer as well, which we called encoder-decoder attention layer. In encoder-decoder attention, query comes from the output from the last decoder layer. However, each position in decoder can only consider every position before current position. This type of masked attention keeps attributes of auto-regressive, and make sure the prediction only depends on those generated output tokens."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
