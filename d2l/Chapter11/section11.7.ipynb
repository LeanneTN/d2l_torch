{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# AdaGrad\n",
    "## Sparse Features and Learning Rates\n",
    "Compared to features that commonly appear, those infrequent features only receive meaningful updates whenever these features occur. Given a decreasing learning rate, we might end up in a situation where the parameters for common features converge rather quickly to their optimal values, whereas for infrequent features we are still short of observing them sufficiently frequently before their optimal values can be determined. In other words, the learning rate either decreases too slowly for frequent features or too quickly for infrequent ones.\n",
    "A possible hack to address this issue would be to count the number of times we see a particular feature and to use this as a clock for adjusting learning rates. That is, rather than choosing a learning rate of the form $\\eta = \\frac{\\eta_0}{\\sqrt{t+c}}$ we could use $\\eta_i = \\frac{\\eta_0}{\\sqrt{s(i, t)+c}}$. Here $s(i, t)$ counts the number of nonzeros for feature $i$ that we have observed up to time $t$.\n",
    "Adagrad addresses this by replacing the rather crude counter $s(i, t+1) = s(i, t) + (\\partial_if(\\mathbf{x}))^2$ as a means to adjust the learning rate.\n",
    "This method has two benefits: first, we no longer need to decide just when a gradient is large enough. Second, it scales automatically with the magnitude of the gradients. Coordinates that routinely correspond to large gradients are scaled down significantly, whereas others with small gradients receive a much more gentle treatment.\n",
    "## Preconditioning\n",
    "We still use the quadratic function $f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^\\top\\mathbf{Q}\\mathbf{x} + \\mathbf{x}^\\top\\mathbf{c} + b$. As we saw in Section 11.6, it is possible to rewrite this problem in terms of its eigendecomposition $\\mathbf{Q} = \\mathbf{U}^\\top\\mathbf{\\Lambda U}$ to arrive at a much simplified problem where each coordinate can be solved individually:\n",
    "$$\n",
    "f(\\mathbf{x}) = \\overline{f}(\\mathbf{\\overline{x}}) = \\frac{1}{2}\\overline{\\mathbf{x}}^\\top\\mathbf{\\Lambda}\\overline{\\mathbf{x}} + \\overline{\\mathbf{c}}^\\top\\overline{\\mathbf{x}} + b\n",
    "$$\n",
    "Here we used $\\overline{\\mathbf{x}} = \\mathbf{Ux}$ and consequently $\\overline{\\mathbf{c}} = \\mathbf{Uc}$. The modified problem has as its minimizer $\\overline{\\mathbf{x}} = -\\mathbf{\\Lambda}\\mathbf{c}$ and minimum value $-\\frac{1}{2}\\overline{\\mathbf{c}}^\\top\\mathbf{\\Lambda}^{-1}\\overline{\\mathbf{c}} + b$. This is much easier to compute as $\\mathbf{\\Lambda}$ is a diagonal matrix containing the eigenvalues of $\\mathbf{Q}$\n",
    "\n",
    "## Algorithm\n",
    "We use variable $\\mathbf{s}_t$ to accumulate past gradient variance as follows.\n",
    "\\begin{equation}\n",
    "\\mathbf{g}_t &= \\partial_\\mathbf{w}l(y_t, f(\\mathbf{x}_t, \\mathbf{w})) \\\\\n",
    "\\mathbf{s}_t &= \\mathbf{s}_{t-1} + \\mathbf{g}_t^2 \\\\\n",
    "\\mathbf{w}_t &= \\mathbf{w}_{t-1} - \\frac{\\eta}{\\sqrt{\\mathbf{s}_t+\\epsilon} \\cdot \\mathbf{g}_t\n",
    "\\end{equation}\n",
    "Just like in the case of momentum we need to keep track of an auxiliary variable, in this case to allow for an individual learning rate per coordinate. This does not increase the cost of Adagrad significantly relative to SGD, simply since the main cost is typically to compute $l(y_t, f(\\mathbf{x}_t, \\mathbf{w})) and its dervative.\n",
    "We still use function $f(\\mathbf{x}) = 0.1x_1^2 + 2x_2^2$ as an example to implement the Adamgrad."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
