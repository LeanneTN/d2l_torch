{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-06T04:48:27.503304500Z",
     "start_time": "2023-07-06T04:48:26.746158600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.2765,  0.1606, -0.1748,  0.2629, -0.0643, -0.1044,  0.1133,  0.3013,\n         -0.0644, -0.0562],\n        [ 0.1345,  0.1428, -0.1031,  0.1285,  0.0788,  0.0250,  0.1236,  0.2887,\n          0.0818,  0.0536]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# block\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\"\"\"\n",
    "In fact, the nn.Sequential() defines a type of special Module, which represents a block in PyTorch.\n",
    "In addition, we use net(X) to invoke our model and receive the output of it. It is the abbr. of net.__call__(X). This forward propagation simply means that it connects every blocks in the list, and use the output of the previous block as the next block\n",
    "\"\"\"\n",
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# construct a class as block\n",
    "class MLP(nn.Module):\n",
    "    # use the parameters of the model to announce layers\n",
    "    def __init__(self):\n",
    "        # invoke the super class of the MLP to process necessary initializations\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.out = nn.Linear(256, 10)\n",
    "    # forward propagation\n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T05:01:20.909439900Z",
     "start_time": "2023-07-06T05:01:20.898466Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.1390,  0.0364, -0.1013,  0.0497, -0.1313, -0.0563, -0.1442,  0.0364,\n         -0.0916, -0.2602],\n        [-0.0373,  0.0113, -0.0031, -0.0342, -0.0101, -0.0164, -0.2264, -0.0075,\n          0.0316, -0.1292]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Firstly, model will invoke the construction function of its superclass, by __init__() we designed in the model. This operation will decline our efforts on writing code of framework over and over again\n",
    "Then we implement two FC layers, which is hidden and out. System will help us with backpropagation and parameters initialization\n",
    "\"\"\"\n",
    "net = MLP()\n",
    "net(X)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T05:07:04.244706700Z",
     "start_time": "2023-07-06T05:07:04.200672200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            #module is an instance of theModule, we save it in the variable _modules, which type is OrderedDict\n",
    "            self._modules[str(idx)] = module\n",
    "\n",
    "    def forward(self, X):\n",
    "        # OrderedDict makes the traversing in the order of adding them\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T05:18:10.693169100Z",
     "start_time": "2023-07-06T05:18:10.674218900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.0972, -0.0164, -0.0068,  0.2696, -0.0763,  0.1803,  0.1163,  0.0224,\n         -0.3395, -0.1903],\n        [-0.0380,  0.1091, -0.0186,  0.1858, -0.1772,  0.1148,  0.1247,  0.0059,\n         -0.3682, -0.0590]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "__init__() will add each module into the orderedDict _modules one by one orderly, and when we use the forward(), each block we added will execute in the order we add them\n",
    "\"\"\"\n",
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net(X)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T05:20:42.953587600Z",
     "start_time": "2023-07-06T05:20:42.939583700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# execute control flow in the forward propagation\n",
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # we don't compute the random weight parameters of gradient, so it remains stable during the training process\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        X = self.linear(X)  # this two FC linear shares parameters\n",
    "        while X.abs().sum() > 1:   # make the L1 norm of the model less than 1, and return the sum of the whole X\n",
    "            X /= 2\n",
    "        return X.sum()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T05:42:17.350389800Z",
     "start_time": "2023-07-06T05:42:17.332463200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.4580, grad_fn=<SumBackward0>)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T05:42:27.374709500Z",
     "start_time": "2023-07-06T05:42:27.362741500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.0491, grad_fn=<SumBackward0>)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mixture and combination\n",
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T05:47:15.103512200Z",
     "start_time": "2023-07-06T05:47:15.078578300Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
