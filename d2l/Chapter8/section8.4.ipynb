{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# RNN\n",
    "We can regard RNN as a Linear Network which has hidden state. Assuming we have small batch of input $\\mathbf{X}_t$ in time slice t, and we have preserved the hidden state of the previous time step $\\mathbf{H}_{t-1}$, and introduced a new weight parameter $\\mathbf{W}_{hh}$, to describe how to use the hidden status. And the calculation of the hidden state is:\n",
    "$$\n",
    "\\mathbf{H}_t = \\phi(\\mathbf{X}_t\\mathbf{W}_{xh}+\\mathbf{H}_{t-1}\\mathbf{W}_{hh}+\\mathbf{b}_h\n",
    "$$\n",
    "![RNN](../statics/imgs/section8.4_fig1.jpg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-11T02:09:03.028809100Z",
     "start_time": "2023-07-11T02:09:00.927382700Z"
    }
   },
   "outputs": [],
   "source": [
    "# RNN\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.3215,  0.4281, -0.9213,  3.1691],\n        [-3.3469,  1.6438,  0.2987,  1.1681],\n        [ 4.7888, -0.9300,  0.2859,  2.5178]])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, W_xh = torch.normal(0, 1, (3, 1)), torch.normal(0, 1, (1, 4))\n",
    "H, W_hh = torch.normal(0, 1, (3, 4)), torch.normal(0, 1, (4, 4))\n",
    "torch.matmul(X, W_xh) + torch.matmul(H, W_hh)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-11T02:09:03.042777300Z",
     "start_time": "2023-07-11T02:09:03.033800700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.3215,  0.4281, -0.9213,  3.1691],\n        [-3.3469,  1.6438,  0.2987,  1.1681],\n        [ 4.7888, -0.9300,  0.2859,  2.5178]])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(torch.cat((X, H), 1), torch.cat((W_xh, W_hh), 0))\n",
    "# concatenate X and H along column (axis 1) and W_xh & W_hh along row (axis 0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-11T02:10:49.233131100Z",
     "start_time": "2023-07-11T02:10:49.210655100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Perplexity\n",
    "Perplexity is an important metric of the language model to value its quality. A fine LM is able to predict what are we going to say in the following token\n",
    "\n",
    "We will use the perspective of the information theory. If we want to compress the text, we could according to the prediction of the next token by current tokens we have. The better our LM is, the smaller number of bits we need when compress the sequence. So we could measure the average entropy of n tokens of one sequence:\n",
    "$$\n",
    "\\frac{1}{n}\\sum_{t=1}^n-logP(x_t|x_{t-1},...,x_1)\n",
    "$$\n",
    "where the P is given by the LM, $x_t$ is the token we observed in time step t. Based on some historical reasons, linguists prefer a metric perplexity to measure it, which is the exp of the formular above\n",
    "$$\n",
    "exp(\\frac{1}{n}\\sum_{t=1}^n-logP(x_t|x_{t-1},...,x_1))\n",
    "$$\n",
    "\n",
    "The best understanding of the perplexity is *the harmonic mean of the number of chosen tokens of the next token*"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
